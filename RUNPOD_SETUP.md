# ุฅุนุฏุงุฏ ุงููุดุฑูุน ุนูู RunPod

ูุฐุง ุงูุฏููู ููุถุญ ููููุฉ ุฅุนุฏุงุฏ ุงููุดุฑูุน ุนูู RunPod ููุงุณุชูุงุฏุฉ ูู GPU ุงูููู.

## ุงููุชุทูุจุงุช ุงูุฃุณุงุณูุฉ

1. **ุญุณุงุจ RunPod** ูุน GPU ูุชุงุญ
2. **Pod** ูุน:
   - GPU: RTX 3090 ุฃู ุฃูุถู (ููุตู ุจู: A100, A6000)
   - RAM: 16GB+ (ููุตู ุจู: 32GB+)
   - Storage: 50GB+ (ูุชุญููู ุงูููุฏููุงุช)

## ุงูุทุฑููุฉ ุงูุฃููู: ุงุณุชุฎุฏุงู Docker (ููุตู ุจู) ๐ณ

### 1. ุฅูุดุงุก Pod ุนูู RunPod

1. ุงุฐูุจ ุฅูู [RunPod](https://www.runpod.io/)
2. ุงุฎุชุฑ **GPU Pod**
3. ุงุฎุชุฑ Template: **Docker** ุฃู **PyTorch**
4. ุงุฎุชุฑ GPU ููุงุณุจ (A100 ููุตู ุจู ููููุฏููุงุช ุงููุจูุฑุฉ)
5. ุงุฎุชุฑ Storage: 50GB+

### 2. ุฑูุน ุงููุดุฑูุน ุฅูู RunPod

#### ุงูุทุฑููุฉ ุฃ: ุฑูุน ูู GitHub

1. ุงุฑูุน ุงููุดุฑูุน ุฅูู GitHub (ุฅุฐุง ูู ููู ููุฌูุฏุงู)
2. ูู RunPod Podุ ุงูุชุญ Terminal
3. ุงุณุชูุณุฎ ุงููุดุฑูุน:
```bash
git clone https://github.com/your-username/your-repo.git
cd your-repo
```

#### ุงูุทุฑููุฉ ุจ: ุฑูุน ุงููููุงุช ูุจุงุดุฑุฉ

1. ูู RunPod Podุ ุงูุชุญ Terminal
2. ุงุณุชุฎุฏู `scp` ุฃู File Manager ูุฑูุน ุงููููุงุช:
```bash
# ูู ุฌูุงุฒู ุงููุญูู
scp -r /path/to/project root@your-pod-ip:/workspace/
```

### 3. ุจูุงุก Docker Image

```bash
cd /workspace/your-repo

# ุจูุงุก ุงูุตูุฑุฉ
docker build -t lecture-assistant:latest .

# ุฃู ุงุณุชุฎุฏุงู docker-compose
docker-compose build
```

### 4. ุฅุนุฏุงุฏ Environment Variables

ุฃูุดุฆ ููู `.env` ูู ุฌุฐุฑ ุงููุดุฑูุน:

```env
# GPU Configuration
CUDA_VISIBLE_DEVICES=0
PYTHON_CMD=python3

# API Keys
GEMINI_API_KEY=your_gemini_api_key_here
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b

# Firebase (ุฅุฐุง ููุช ุชุณุชุฎุฏูู)
GOOGLE_APPLICATION_CREDENTIALS=/app/firebase-service-account.json

# Server
NODE_ENV=production
PORT=5000
```

### 5. ุชุดุบูู Container

```bash
# ุทุฑููุฉ 1: ุงุณุชุฎุฏุงู docker run
docker run -d \
  --name lecture-assistant \
  --gpus all \
  -p 5000:5000 \
  --env-file .env \
  -v $(pwd)/firebase-service-account.json:/app/firebase-service-account.json:ro \
  lecture-assistant:latest

# ุทุฑููุฉ 2: ุงุณุชุฎุฏุงู docker-compose (ุฃุณูู)
docker-compose up -d
```

### 6. ุงูุชุญูู ูู ุงูุชุดุบูู

```bash
# ุงูุชุญูู ูู ุงูู logs
docker logs lecture-assistant

# ุงูุชุญูู ูู Health endpoint
curl http://localhost:5000/api/health

# ุงูุชุญูู ูู GPU
docker exec lecture-assistant nvidia-smi
```

### 7. ุงููุตูู ุฅูู ุงูุชุทุจูู

- **ูู ุฏุงุฎู RunPod**: `http://localhost:5000`
- **ูู ุฎุงุฑุฌ RunPod**: ุงุณุชุฎุฏู RunPod's Public URL ุฃู Tunnel

## ุงูุทุฑููุฉ ุงูุซุงููุฉ: ุงูุชุซุจูุช ุงููุฏูู (ุจุฏูู Docker)

### 1. ุฅูุดุงุก Pod ุนูู RunPod

1. ุงุฐูุจ ุฅูู [RunPod](https://www.runpod.io/)
2. ุงุฎุชุฑ **GPU Pod**
3. ุงุฎุชุฑ Template: **PyTorch** ุฃู **CUDA**
4. ุงุฎุชุฑ GPU ููุงุณุจ (A100 ููุตู ุจู ููููุฏููุงุช ุงููุจูุฑุฉ)
5. ุงุฎุชุฑ Storage: 50GB+

### 2. ุชุซุจูุช ุงููุชุทูุจุงุช

```bash
# ุชุญุฏูุซ ุงููุธุงู
sudo apt-get update
sudo apt-get upgrade -y

# ุชุซุจูุช Node.js
curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
sudo apt-get install -y nodejs

# ุชุซุจูุช Python dependencies
pip install --upgrade pip
pip install faster-whisper torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# ุชุซุจูุช ูุชุทูุจุงุช ุงููุดุฑูุน
pip install -r requirements.txt

# ุชุซุจูุช Node.js dependencies
npm install
```

### 3. ุงูุชุญูู ูู GPU

```bash
# ุงูุชุญูู ูู ุชููุฑ CUDA
python3 -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"

# ุงูุชุญูู ูู faster-whisper
python3 -c "from faster_whisper import WhisperModel; print('faster-whisper installed successfully')"
```

### 4. ุฅุนุฏุงุฏ ุงููุชุบูุฑุงุช ุงูุจูุฆูุฉ

ุฃูุดุฆ ููู `.env`:

```env
# GPU Configuration
CUDA_VISIBLE_DEVICES=0

# Python Path (if needed)
PYTHON_CMD=python3

# Other settings
GEMINI_API_KEY=your_key_here
OLLAMA_URL=http://localhost:11434
# Qwen Model - Choose the best for your GPU:
# qwen2.5:32b (Best quality - 20GB+ VRAM - Recommended for A100/A6000)
# qwen2.5:14b (Great quality - 10GB+ VRAM - Recommended for RTX 3090/4090)
# qwen2.5:7b (Good quality - 5GB+ VRAM - Minimum recommended)
OLLAMA_MODEL=qwen2.5:32b
```

### 5. ุจูุงุก ุงูุชุทุจูู

```bash
npm run build
```

### 6. ุชุดุบูู ุงูุชุทุจูู

```bash
# Production mode
npm start

# ุฃู Development mode
npm run dev
```

### 7. ุงุฎุชุจุงุฑ ุงูุชุญููู ุงูุตูุชู

```bash
# ุงุฎุชุจุงุฑ ุจุณูุท
python3 server/scripts/transcribe_audio.py /path/to/audio.mp3 large-v3 None cuda
```

## ุงูุฅุนุฏุงุฏุงุช ุงูููุตู ุจูุง

### ููููุฏููุงุช ุงููุจูุฑุฉ (large-v3):

- **GPU**: A100 40GB ุฃู ุฃูุถู
- **RAM**: 32GB+
- **Compute Type**: float16 (ุงูุชุฑุงุถู)
- **Beam Size**: 5

### ููููุฏููุงุช ุงููุชูุณุทุฉ (medium):

- **GPU**: RTX 3090 ุฃู ุฃูุถู
- **RAM**: 16GB+
- **Compute Type**: float16
- **Beam Size**: 5

## ุชุญุณููุงุช ุงูุฃุฏุงุก

### 1. ุงุณุชุฎุฏุงู float16 ููู GPU

ุงููุดุฑูุน ูุณุชุฎุฏู ุชููุงุฆูุงู `float16` ููู GPU ููุญุตูู ุนูู ุฃูุถู ุฃุฏุงุก.

### 2. ุชุญููู ุงูููุฏูู ูุณุจูุงู

ุนูุฏ ุฃูู ุงุณุชุฎุฏุงูุ ุณูุชู ุชุญููู ุงูููุฏูู ุชููุงุฆูุงู. ููููู ุชุญูููู ูุณุจูุงู:

```python
from faster_whisper import WhisperModel
model = WhisperModel("large-v3", device="cuda", compute_type="float16")
```

### 3. ุงุณุชุฎุฏุงู Batch Processing

ูููููุงุช ุงููุชุนุฏุฏุฉุ ููููู ูุนุงูุฌุชูุง ุจุดูู ูุชูุงุฒู.

## ุงุณุชูุดุงู ุงูุฃุฎุทุงุก

### ุงููุดููุฉ: CUDA not available

**ุงูุญู:**
```bash
# ุงูุชุญูู ูู CUDA
nvidia-smi

# ูู Docker
docker exec lecture-assistant nvidia-smi

# ุฅุนุงุฏุฉ ุชุซุจูุช PyTorch ูุน CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### ุงููุดููุฉ: Out of Memory

**ุงูุญู:**
- ุงุณุชุฎุฏู ููุฏูู ุฃุตุบุฑ (medium ุจุฏูุงู ูู large-v3)
- ุงุณุชุฎุฏู `int8_float16` ุจุฏูุงู ูู `float16`
- ููู `beam_size` ุฅูู 3

### ุงููุดููุฉ: Model download failed

**ุงูุญู:**
```bash
# ุชุญููู ุงูููุฏูู ูุฏููุงู
python3 -c "from faster_whisper import WhisperModel; WhisperModel('large-v3', device='cuda')"
```

### ุงููุดููุฉ: Docker build failed

**ุงูุญู:**
```bash
# ุชูุธูู Docker cache
docker system prune -a

# ุฅุนุงุฏุฉ ุงูุจูุงุก ุจุฏูู cache
docker build --no-cache -t lecture-assistant:latest .
```

### ุงููุดููุฉ: Port already in use

**ุงูุญู:**
```bash
# ุชุบููุฑ PORT ูู .env
PORT=8080

# ุฃู ุฅููุงู ุงูุนูููุฉ ุงูุชู ุชุณุชุฎุฏู ุงููููุฐ
lsof -ti:5000 | xargs kill -9
```

## ููุงุญุธุงุช ูููุฉ

1. **ุงูููุฏูู ุงูุงูุชุฑุงุถู**: `large-v3` (ุงูุฃูุถู ุฏูุฉ)
2. **ุงูุฌูุงุฒ ุงูุงูุชุฑุงุถู**: GPU (cuda)
3. **Compute Type**: float16 ููู GPU (ุฃูุถู ุฃุฏุงุก)
4. **ุงูุชุญููู ุงูุชููุงุฆู**: ุงูููุฏููุงุช ุชูุญููู ุชููุงุฆูุงู ุนูุฏ ุฃูู ุงุณุชุฎุฏุงู
5. **Docker**: ููุตู ุจู ููุณูููุฉ ูุงูุงุณุชูุฑุงุฑ
6. **Storage**: ุชุฃูุฏ ูู ูุฌูุฏ ูุณุงุญุฉ ูุงููุฉ ููููุฏููุงุช (~3-5GB ููู ููุฏูู)

## ุงูุฃุฏุงุก ุงููุชููุน

### ุนูู A100 40GB:

- **large-v3**: ~2-5x ุฃุณุฑุน ูู ุงูููุช ุงููุนูู ููุตูุช
- **medium**: ~5-10x ุฃุณุฑุน ูู ุงูููุช ุงููุนูู ููุตูุช
- **base**: ~10-20x ุฃุณุฑุน ูู ุงูููุช ุงููุนูู ููุตูุช

### ุนูู RTX 3090:

- **large-v3**: ~1-3x ุฃุณุฑุน ูู ุงูููุช ุงููุนูู ููุตูุช
- **medium**: ~3-5x ุฃุณุฑุน ูู ุงูููุช ุงููุนูู ููุตูุช
- **base**: ~5-10x ุฃุณุฑุน ูู ุงูููุช ุงููุนูู ููุตูุช

## ูููุงุช Docker ุงููุชููุฑุฉ

- `Dockerfile`: ููู ุจูุงุก Docker ุงูุฑุฆูุณู
- `docker-compose.yml`: ููู ุชูููู Docker Compose
- `.dockerignore`: ูููุงุช ูุณุชุจุนุฏุฉ ูู Docker build
- `startup.sh`: ุณูุฑูุจุช ุจุฏุก ุงูุชุดุบูู

## ุงูุฏุนู

ุฅุฐุง ูุงุฌูุช ุฃู ูุดุงููุ ุฑุงุฌุน:
- [faster-whisper Documentation](https://github.com/guillaumekln/faster-whisper)
- [RunPod Documentation](https://docs.runpod.io/)
- [Docker Documentation](https://docs.docker.com/)
